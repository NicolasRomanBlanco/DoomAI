{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed08ee2-f7aa-417a-ba0f-e1189c7debac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vizdoom\n",
      "  Downloading vizdoom-1.2.3-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\nicol\\onedrive\\desktop\\nico\\estudios\\pythonvenv\\jupyter_playground\\lib\\site-packages (from vizdoom) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in c:\\users\\nicol\\onedrive\\desktop\\nico\\estudios\\pythonvenv\\jupyter_playground\\lib\\site-packages (from vizdoom) (0.29.1)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\nicol\\onedrive\\desktop\\nico\\estudios\\pythonvenv\\jupyter_playground\\lib\\site-packages (from vizdoom) (2.5.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\nicol\\onedrive\\desktop\\nico\\estudios\\pythonvenv\\jupyter_playground\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\nicol\\onedrive\\desktop\\nico\\estudios\\pythonvenv\\jupyter_playground\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (4.10.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\nicol\\onedrive\\desktop\\nico\\estudios\\pythonvenv\\jupyter_playground\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\nicol\\onedrive\\desktop\\nico\\estudios\\pythonvenv\\jupyter_playground\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (7.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\nicol\\onedrive\\desktop\\nico\\estudios\\pythonvenv\\jupyter_playground\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium>=0.28.0->vizdoom) (3.17.0)\n",
      "Downloading vizdoom-1.2.3-cp39-cp39-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/15.5 MB 11.4 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 1.0/15.5 MB 22.5 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 1.0/15.5 MB 22.5 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 2.1/15.5 MB 13.3 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 2.1/15.5 MB 13.3 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.1/15.5 MB 11.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.1/15.5 MB 11.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.1/15.5 MB 11.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.2/15.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.2/15.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.2/15.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.2/15.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.2/15.5 MB 8.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.2/15.5 MB 8.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.2/15.5 MB 8.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.2/15.5 MB 8.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.2/15.5 MB 8.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.3/15.5 MB 7.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.3/15.5 MB 7.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.3/15.5 MB 7.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.3/15.5 MB 7.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.3/15.5 MB 7.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.3/15.5 MB 7.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.3/15.5 MB 7.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.3/15.5 MB 7.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.4/15.5 MB 7.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.4/15.5 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.4/15.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.4/15.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.4/15.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.4/15.5 MB 7.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.5/15.5 MB 7.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.5/15.5 MB 7.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.5/15.5 MB 7.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.5/15.5 MB 7.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.5/15.5 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.5/15.5 MB 6.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.5/15.5 MB 6.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.6/15.5 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.6/15.5 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.6/15.5 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.6/15.5 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.6/15.5 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.6/15.5 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.6/15.5 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.6/15.5 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.6/15.5 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.6/15.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.6/15.5 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.6/15.5 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.6/15.5 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.6/15.5 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.6/15.5 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.6/15.5 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.6/15.5 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.7/15.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.7/15.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.7/15.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.7/15.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.5 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.5 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 4.5 MB/s eta 0:00:00\n",
      "Installing collected packages: vizdoom\n",
      "Successfully installed vizdoom-1.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install vizdoom\n",
    "!pip install gym==0.21\n",
    "!cd github & git clone https://github.com/Farama-Foundation/ViZDoom\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install stable-baselines3[extra]==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e4b1423-315b-4efe-9248-72d8763919d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vizdoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# Import numpy for identity matrix\n",
    "import numpy as np\n",
    "# Import environment base class from OpenAI Gym\n",
    "from gym import Env\n",
    "# Import gym spaces\n",
    "from gym.spaces import Discrete, Box \n",
    "# Import opencv\n",
    "import cv2\n",
    "# Import Environment checker\n",
    "from stable_baselines3.common import env_checker\n",
    "# Import os for file nav\n",
    "import os\n",
    "# Import callback class from sb3\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "# Import PPO for training\n",
    "from stable_baselines3 import PPO\n",
    "# Import eval policy to test agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c830d48-517f-47bf-b63c-1324c2f6ab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Create ViZDoom OpenAI Gym environment\n",
    "class VizDoomGym(Env):\n",
    "    # Function that is called when whe start the env\n",
    "    def __init__(self, render=False):\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        # Setup the game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('github/ViZDoom/scenarios/defend_the_center.cfg')\n",
    "\n",
    "        # Render frame Logic\n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        \n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create the action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100,160,1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "    # This is how we take a step in the environment\n",
    "    def step(self, action):\n",
    "        # Specify action and take step \n",
    "        actions = np.identity(3)\n",
    "        reward = self.game.make_action(actions[action], 4) \n",
    "        \n",
    "        # Get all the other stuff we need to retun \n",
    "        if self.game.get_state(): \n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)\n",
    "            ammo = self.game.get_state().game_variables[0]\n",
    "            info = ammo\n",
    "        else: \n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0 \n",
    "        \n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state, reward, done, info \n",
    "        \n",
    "    # Define how to render the game or environment\n",
    "    def render(self):\n",
    "        pass\n",
    "        \n",
    "    # What happens when we start a new game\n",
    "    def reset(self):\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state)\n",
    "        \n",
    "    # Grayscale the game frame and resize it\n",
    "    def grayscale(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160,1))\n",
    "        return state\n",
    "        \n",
    "    # Close doen the game\n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "            \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "                                      \n",
    "        return True\n",
    "        \n",
    "CHECKPOINT_DIR = './train/train_defend_the_center'\n",
    "LOG_DIR = './logs/log_defend_the_center'\n",
    "\n",
    "# Setup model saving callback\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)\n",
    "\n",
    "# Non render environment\n",
    "env = VizDoomGym()\n",
    "# Create the model\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.0001, n_steps=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c5d455e-da99-4148-b93e-a2017af5091c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/log_defend_the_center\\PPO_2\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 78.7     |\n",
      "|    ep_rew_mean     | 0.154    |\n",
      "| time/              |          |\n",
      "|    fps             | 109      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 84.7        |\n",
      "|    ep_rew_mean          | 0.531       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014549976 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0382      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0568      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00966    |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 93.7        |\n",
      "|    ep_rew_mean          | 1.17        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010087533 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00236    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 1.99        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012882119 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.563       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0213     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0334     |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 108         |\n",
      "|    ep_rew_mean          | 2.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015473218 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.998      |\n",
      "|    explained_variance   | 0.589       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0375     |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 112         |\n",
      "|    ep_rew_mean          | 3.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019889355 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.95       |\n",
      "|    explained_variance   | 0.585       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0423     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 118         |\n",
      "|    ep_rew_mean          | 3.87        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 314         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016652972 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.897      |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0205     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0372     |\n",
      "|    value_loss           | 0.168       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 121         |\n",
      "|    ep_rew_mean          | 4.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 361         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018176604 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.87       |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0445     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 127        |\n",
      "|    ep_rew_mean          | 4.9        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 90         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 409        |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01996021 |\n",
      "|    clip_fraction        | 0.204      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.839     |\n",
      "|    explained_variance   | 0.732      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0109     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0455    |\n",
      "|    value_loss           | 0.15       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 131         |\n",
      "|    ep_rew_mean          | 5.22        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 454         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025284573 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.786      |\n",
      "|    explained_variance   | 0.752       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0435     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0406     |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 142         |\n",
      "|    ep_rew_mean          | 5.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 500         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022158531 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.776      |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00929    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0411     |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 148         |\n",
      "|    ep_rew_mean          | 6.28        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 547         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027809124 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.754      |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0163     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0422     |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 154         |\n",
      "|    ep_rew_mean          | 6.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 594         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026768077 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.703      |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0119      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 159         |\n",
      "|    ep_rew_mean          | 7.21        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 641         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027350178 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.68       |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00434    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 167         |\n",
      "|    ep_rew_mean          | 7.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 688         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027550755 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.675      |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00586    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.04       |\n",
      "|    value_loss           | 0.186       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 168         |\n",
      "|    ep_rew_mean          | 8.11        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 735         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027733356 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.641      |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.028      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0398     |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 178         |\n",
      "|    ep_rew_mean          | 8.81        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 782         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026622167 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00504    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0359     |\n",
      "|    value_loss           | 0.209       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 183         |\n",
      "|    ep_rew_mean          | 9.09        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 831         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029331561 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.641      |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0184      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 184         |\n",
      "|    ep_rew_mean          | 9.23        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 878         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025905505 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0488     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0421     |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 189         |\n",
      "|    ep_rew_mean          | 9.58        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 925         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031647235 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.616      |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0302     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0408     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 197         |\n",
      "|    ep_rew_mean          | 10.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 972         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027927723 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.612      |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.019      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | 10.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 1020        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030004302 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.584      |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0466     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 202         |\n",
      "|    ep_rew_mean          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 1066        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030706301 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0158     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 201         |\n",
      "|    ep_rew_mean          | 10.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 1116        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034645427 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0141     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 203         |\n",
      "|    ep_rew_mean          | 11.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 1166        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030763786 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0369     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 196         |\n",
      "|    ep_rew_mean          | 10.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 1215        |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034851648 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.019      |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:310\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    299\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     reset_num_timesteps: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:247\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    243\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 247\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:166\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 166\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\stable_baselines3\\common\\policies.py:588\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03mForward pass in all the networks (actor and critic)\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;124;03m:return: action, value and log probability of the action\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[1;32m--> 588\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    589\u001b[0m latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor(features)\n\u001b[0;32m    590\u001b[0m \u001b[38;5;66;03m# Evaluate the values for the given observations\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\stable_baselines3\\common\\policies.py:129\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo features extractor was set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m preprocessed_obs \u001b[38;5;241m=\u001b[39m preprocess_obs(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, normalize_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_images)\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_obs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:93\u001b[0m, in \u001b[0;36mNatureCNN.forward\u001b[1;34m(self, observations)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, observations: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.learn(total_timesteps=1000000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19fd4dc0-c27e-4385-a047-b1042459d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload from disc\n",
    "model = PPO.load('./train/train_defend_the_center/best_model_110000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7199b313-793d-4662-833c-7dcea49edbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a rendered environment\n",
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36ba831-c63f-40d5-aeb4-6c735e6ef86a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\OneDrive\\Desktop\\Nico\\Estudios\\PythonVenv\\jupyter_playground\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Evaluate mean reward for 100 games\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bca9b7a-2fa7-4091-80fe-5e750523aa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for episode 0 is 13.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n\u001b[0;32m      8\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal reward for episode \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(episode, total_reward))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate the total reward for 10 episodes\n",
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.20)\n",
    "        total_reward += reward\n",
    "    print('Total reward for episode {} is {}'.format(episode, total_reward))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eba480-35a1-4492-b85f-3962b999d18a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
